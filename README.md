# Pipeline de Processamento de Dados com PySpark e Streamlit

Este projeto implementa um pipeline de processamento de dados em "tempo real" (streaming) para an√°lise de transa√ß√µes financeiras. Ele simula o fluxo de dados atrav√©s de sockets, processa as informa√ß√µes utilizando **Apache Spark (PySpark)** e exibe m√©tricas din√¢micas em um dashboard interativo constru√≠do com **Streamlit**.

## üè´ Contexto Acad√™mico

Este c√≥digo foi desenvolvido como parte das atividades avaliativas da disciplina de **Computa√ß√£o Paralela e Distribu√≠da (CPD)** da **Universidade do Estado do Rio Grande do Norte (UERN)**, semestre 2025.2.

O objetivo principal √© demonstrar as capacidades do Spark em um cen√°rio de pipeline de dados simplificado, abordando conceitos de streaming, agrega√ß√£o e visualiza√ß√£o de dados.

* **Professor:** Alysson Oliveira
* **Alunos:**
    * Eduardo Marinho
    * Paulo S√©rgio
    * Vinicius Eduardo
    * Marcos Eduardo
    * Luiz Henrique

---

## ‚öôÔ∏è Arquitetura do Projeto

O fluxo de dados funciona da seguinte maneira:

1.  **Fonte de Dados (Simula√ß√£o):** Um servidor Python l√™ um arquivo CSV (`transa√ß√µes_2000.csv`) e envia cada linha sequencialmente via **Socket TCP** (localhost:9999).
2.  **Processamento (ETL):** O PySpark (Structured Streaming) conecta-se ao socket, recebe os dados brutos, realiza a limpeza, formata√ß√£o e agrega√ß√µes (somas, contagens, m√°ximos/m√≠nimos).
3.  **Persist√™ncia Intermedi√°ria:** O Spark escreve os resultados processados periodicamente em arquivos CSV locais (`dash_*.csv`).
4.  **Visualiza√ß√£o:** O Streamlit l√™ esses arquivos CSV em loop e atualiza os gr√°ficos e KPIs na tela do usu√°rio.

```mermaid
graph LR
    A[transa√ß√µes_2000.csv] -->|L√™| B(servidor_transacoes.py)
    B -->|Socket :9999| C(processador_spark.py)
    C -->|Processa & Salva| D[Arquivos CSV Tempor√°rios]
    D -->|L√™| E(dashboard.py)
    E -->|Exibe| F[Browser User]
```

-----

## üõ†Ô∏è Pr√©-requisitos

Para executar este projeto, voc√™ precisar√° ter instalado em sua m√°quina:

1.  **Python 3.8+**
2.  **Java 8 ou 11** (Obrigat√≥rio para rodar o Apache Spark).
      * *Nota: Certifique-se de que a vari√°vel de ambiente `JAVA_HOME` est√° configurada.*
3.  **Bibliotecas Python:**
    Instale as depend√™ncias executando:


```bash
pip install -r requirements.txt
```

-----

## üöÄ Como Executar

Como o sistema simula um ambiente distribu√≠do, voc√™ precisar√° de **3 terminais** abertos simultaneamente. Siga a ordem abaixo:

### Passo 1: Iniciar o Servidor de Transa√ß√µes (Produtor)

Este script vai ler o CSV e aguardar uma conex√£o na porta 9999.

**No Terminal 1:**

```bash
python servidor_transacoes.py
```

*Aguarde a mensagem: "Aguardando conex√£o do Spark na porta 9999..."*

### Passo 2: Iniciar o Processador Spark (Consumidor)

Este script conecta no servidor, processa os dados e gera os arquivos de sa√≠da.

**No Terminal 2:**

```bash
python processador_spark.py
```

*Assim que iniciar, voc√™ ver√° no Terminal 1 que a conex√£o foi estabelecida e os dados come√ßar√£o a ser enviados.*

### Passo 3: Iniciar o Dashboard (Frontend)

Este script sobe a interface visual.

**No Terminal 3:**

```bash
streamlit run dashboard.py
```

*O navegador abrir√° automaticamente no endere√ßo (geralmente) `http://localhost:8501`.*

-----

## üìÇ Estrutura de Arquivos

  * `transa√ß√µes_2000.csv`: Dataset contendo 2000 registros de transa√ß√µes simuladas.
  * `servidor_transacoes.py`: Script socket que simula o envio de dados em streaming.
  * `processador_spark.py`: Script principal do PySpark que faz o ETL e c√°lculos estat√≠sticos.
  * `dashboard.py`: Aplica√ß√£o web para visualiza√ß√£o dos dados.
  * `dash_*.csv`: Arquivos gerados automaticamente pelo Spark contendo os dados processados (KPIs, An√°lise Mensal, Institui√ß√£o, etc).

-----

## ‚ö†Ô∏è Notas Importantes

  * **Erros de Conex√£o:** Se voc√™ encerrar o `servidor_transacoes.py`, o `processador_spark.py` ir√° parar. Para reiniciar, comece sempre pelo Passo 1.
  * **Porta em Uso:** Se der erro de "Address already in use", aguarde um minuto para o sistema operacional liberar a porta 9999 ou altere a porta nos arquivos `servidor_transacoes.py` e `processador_spark.py`.